{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "00fdf768",
   "metadata": {},
   "source": [
    "#  22 september update...badhu navu kryu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b34beac",
   "metadata": {},
   "source": [
    "awesome. let’s kick this off clean with your env name **mllocalag** and quality-first Jina v3 + OCR. below is a tight, copy-paste runbook. (you can keep your current code; I only add two tiny helpers and 3–4 lines in ingest/embed.)\n",
    "\n",
    "---\n",
    "\n",
    "# 1) one-time cleanup (safe)\n",
    "\n",
    "```powershell\n",
    "# stop/remove qdrant + old volumes (ignore errors if not found)\n",
    "docker rm -f qdrant 2>$null\n",
    "docker volume ls | findstr qdrant\n",
    "# if a qdrant volume appears, remove it:\n",
    "# docker volume rm multilingual-ragbot_qdrant_storage\n",
    "\n",
    "# clear python caches & old HF cache (we’ll use a short path next)\n",
    "Get-ChildItem -Recurse -Force -Include __pycache__ | Remove-Item -Recurse -Force -ErrorAction SilentlyContinue\n",
    "Get-ChildItem -Recurse -Force -Include *.pyc | Remove-Item -Force -ErrorAction SilentlyContinue\n",
    "Remove-Item -Recurse -Force \"$env:USERPROFILE\\.cache\\huggingface\" -ErrorAction SilentlyContinue\n",
    "\n",
    "# fresh metadata (we’ll regenerate on ingest)\n",
    "Remove-Item -Recurse -Force .\\data\\metadata -ErrorAction SilentlyContinue\n",
    "New-Item -ItemType Directory -Force -Path .\\data\\metadata | Out-Null\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2) new environment (name: **mllocalag**)\n",
    "\n",
    "```powershell\n",
    "conda create -n mllocalag python=3.10 -y\n",
    "conda activate mllocalag\n",
    "```\n",
    "\n",
    "**short HF cache (prevents the old Windows long-path crash)**\n",
    "\n",
    "```powershell\n",
    "[Environment]::SetEnvironmentVariable(\"HF_HOME\",\"C:\\hf\",\"User\")\n",
    "[Environment]::SetEnvironmentVariable(\"HF_HUB_CACHE\",\"C:\\hf\\hub\",\"User\")\n",
    "[Environment]::SetEnvironmentVariable(\"TRANSFORMERS_CACHE\",\"C:\\hf\\tf\",\"User\")\n",
    "# close this terminal, open a new one, then:\n",
    "conda activate mllocalag\n",
    "```\n",
    "\n",
    "**GPU (recommended): install PyTorch CUDA (choose ONE path)**\n",
    "\n",
    "* pip (CUDA 12.1):\n",
    "\n",
    "```powershell\n",
    "pip install --index-url https://download.pytorch.org/whl/cu121 torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "* or conda:\n",
    "\n",
    "```powershell\n",
    "conda install -y pytorch pytorch-cuda=12.1 -c pytorch -c nvidia\n",
    "```\n",
    "\n",
    "**base deps**\n",
    "\n",
    "```powershell\n",
    "pip install -U pip wheel\n",
    "pip install -r requirements.txt\n",
    "pip install -U \"sentence-transformers>=3.0.1\" \"transformers>=4.44.0\" \"huggingface_hub>=0.24\" accelerate safetensors\n",
    "pip install ocrmypdf pdfplumber pytesseract pillow\n",
    "```\n",
    "\n",
    "> Make sure system Tesseract is installed with **deu** + **eng** traineddata. (OCRmyPDF will use it.)\n",
    "\n",
    "---\n",
    "\n",
    "# 3) config (quality-first Jina v3)\n",
    "\n",
    "Open **core/config.py** and confirm:\n",
    "\n",
    "```python\n",
    "CFG.embed_model        = \"jinaai/jina-embeddings-v3\"\n",
    "CFG.embed_dim          = 1024                # full quality; we can crop to 512 later\n",
    "CFG.embed_doc_prefix   = \"search_document: \"\n",
    "CFG.embed_query_prefix = \"search_query: \"\n",
    "CFG.qdrant_url         = \"http://127.0.0.1:6333\"\n",
    "CFG.qdrant_collection  = \"tender_docs_jina-v3_d1024_fresh\"\n",
    "CFG.extract_dir        = \"data/extract\"\n",
    "CFG.ocr_cache_dir      = \"data/ocr_cache\"\n",
    "CFG.ocr_langs          = \"deu+eng\"\n",
    "CFG.ocr_quality        = \"quality\"          # set \"speed\" later if needed\n",
    "CFG.ocr_jobs           = 0                   # auto = cpu_count()-1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4) add two helpers (copy-paste)\n",
    "\n",
    "## 4.1 `core/text_cleaning.py`\n",
    "\n",
    "```python\n",
    "import re, unicodedata\n",
    "URL   = re.compile(r'https?://\\S+|www\\.\\S+', re.I)\n",
    "EMAIL = re.compile(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b')\n",
    "WS    = re.compile(r'[ \\t\\r\\f\\v]+')\n",
    "CTRL  = re.compile(r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F\\x7F]')\n",
    "NOISE = re.compile(r'[^\\w\\s\\.\\,\\;\\:\\!\\?\\(\\)\\[\\]\\{\\}\\'\\\"\\-\\/_\\#\\%\\&\\+\\*\\=\\<\\>\\@]')\n",
    "NL3   = re.compile(r'\\n{3,}')\n",
    "SP2   = re.compile(r' {2,}')\n",
    "\n",
    "def _nfkc(t): \n",
    "    t = unicodedata.normalize(\"NFKC\", t).replace(\"\\u00A0\",\" \")\n",
    "    return (t.replace(\"\\u2018\",\"'\").replace(\"\\u2019\",\"'\")\n",
    "             .replace(\"\\u201C\",'\"').replace(\"\\u201D\",'\"')\n",
    "             .replace(\"\\u2013\",\"-\").replace(\"\\u2014\",\"-\"))\n",
    "\n",
    "def _fix_hyphens(t): return re.sub(r'-\\s*\\n\\s*','-',t)\n",
    "\n",
    "def clean_text(text: str) -> str:\n",
    "    if not text: return \"\"\n",
    "    text = _nfkc(text)\n",
    "    text = CTRL.sub(\" \", text)\n",
    "    text = URL.sub(\" \", text)\n",
    "    text = EMAIL.sub(\" \", text)\n",
    "    text = _fix_hyphens(text)\n",
    "    text = WS.sub(\" \", text)\n",
    "    text = NOISE.sub(\" \", text)\n",
    "    text = SP2.sub(\" \", text).strip()\n",
    "    text = NL3.sub(\"\\n\\n\", text)\n",
    "    return text\n",
    "```\n",
    "\n",
    "## 4.2 `core/ocr.py`\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "import hashlib, json, subprocess, tempfile, shutil\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, Any\n",
    "import pdfplumber\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "import fitz  # PyMuPDF\n",
    "\n",
    "from core.config import CFG\n",
    "\n",
    "@dataclass\n",
    "class OcrResult:\n",
    "    pdf_path: Path\n",
    "    metrics: Dict[str, Any]\n",
    "\n",
    "def _jobs():\n",
    "    import os\n",
    "    n = os.cpu_count() or 4\n",
    "    return CFG.ocr_jobs if CFG.ocr_jobs and CFG.ocr_jobs>0 else max(1, n-1)\n",
    "\n",
    "def _profile_args():\n",
    "    if CFG.ocr_quality == \"speed\":\n",
    "        return [\"--skip-text\",\"--optimize\",\"1\",\"--rotate-pages\",\"--jobs\",str(_jobs())]\n",
    "    return [\"--redo-ocr\",\"--deskew\",\"--remove-background\",\"--rotate-pages\",\n",
    "            \"--tesseract-timeout\",\"0\",\"--optimize\",\"2\",\"--tesseract-config\",\"textonly_pdf=1\",\n",
    "            \"--jobs\",str(_jobs())]\n",
    "\n",
    "def _estimate_native_chars(pdf: Path) -> int:\n",
    "    total = 0\n",
    "    with pdfplumber.open(str(pdf)) as d:\n",
    "        for p in d.pages[:5]:\n",
    "            total += len((p.extract_text() or \"\").strip())\n",
    "    return total\n",
    "\n",
    "def _collect_metrics(pdf: Path, applied: bool, note: str=\"\") -> Dict[str,Any]:\n",
    "    pages=[]\n",
    "    with pdfplumber.open(str(pdf)) as d:\n",
    "        for i,p in enumerate(d.pages):\n",
    "            pages.append({\"page\":i+1,\"len\":len((p.extract_text() or \"\"))})\n",
    "    return {\"ocr_applied\":applied,\"note\":note,\"pages\":pages,\"total_chars\":sum(p[\"len\"] for p in pages)}\n",
    "\n",
    "def _tess_img_to_text(img_path: Path, langs: str, psm: str) -> str:\n",
    "    img = Image.open(str(img_path))\n",
    "    try:\n",
    "        cfg = f\"--oem 1 --psm {psm}\"\n",
    "        d = pytesseract.image_to_data(img, lang=langs, config=cfg, output_type=pytesseract.Output.DICT)\n",
    "        words = d.get(\"text\", [])\n",
    "        return \" \".join(w for w in words if w and w.strip())\n",
    "    finally:\n",
    "        img.close()\n",
    "\n",
    "def _pytess_fallback_to_pdf(src: Path, langs: str) -> Path:\n",
    "    doc = fitz.open(str(src))\n",
    "    out = fitz.open()\n",
    "    tmpdir = Path(tempfile.mkdtemp())\n",
    "    for i,page in enumerate(doc):\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        img = tmpdir / f\"p{i:04d}.png\"\n",
    "        pix.save(str(img))\n",
    "        txt = _tess_img_to_text(img, langs, psm=\"6\")\n",
    "        if len(txt.strip())<30:\n",
    "            txt = _tess_img_to_text(img, langs, psm=\"11\")\n",
    "        rect = fitz.Rect(0,0,pix.width,pix.height)\n",
    "        new = out.new_page(width=rect.width, height=rect.height)\n",
    "        new.insert_textbox(rect, txt)\n",
    "    out_path = tmpdir / \"fallback_ocr.pdf\"\n",
    "    out.save(str(out_path)); out.close(); doc.close()\n",
    "    return out_path\n",
    "\n",
    "def ocr_pdf_if_needed(src_pdf: Path) -> OcrResult:\n",
    "    src_pdf = Path(src_pdf).resolve()\n",
    "    Path(CFG.ocr_cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "    key = hashlib.sha256((src_pdf.read_bytes()+str(CFG.ocr_quality+CFG.ocr_langs).encode())).hexdigest()[:16]\n",
    "    out_pdf = Path(CFG.ocr_cache_dir) / f\"{src_pdf.stem}.{key}.ocr.pdf\"\n",
    "    metrics_json = Path(CFG.ocr_cache_dir) / f\"{src_pdf.stem}.{key}.metrics.json\"\n",
    "\n",
    "    if out_pdf.exists() and metrics_json.exists():\n",
    "        return OcrResult(out_pdf, json.loads(metrics_json.read_text(encoding=\"utf-8\")))\n",
    "\n",
    "    if _estimate_native_chars(src_pdf) > 500:\n",
    "        m = {\"native_text_chars\":True,\"ocr_applied\":False,\"pages\":[],\"total_chars\":0}\n",
    "        metrics_json.write_text(json.dumps(m, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        return OcrResult(src_pdf, m)\n",
    "\n",
    "    cmd = [\"ocrmypdf\",\"-l\",CFG.ocr_langs,*_profile_args(),str(src_pdf),str(out_pdf)]\n",
    "    try:\n",
    "        subprocess.run(cmd, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        m = _collect_metrics(out_pdf, True)\n",
    "        metrics_json.write_text(json.dumps(m, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        return OcrResult(out_pdf, m)\n",
    "    except Exception:\n",
    "        tmp_pdf = _pytess_fallback_to_pdf(src_pdf, CFG.ocr_langs)\n",
    "        shutil.move(tmp_pdf, out_pdf)\n",
    "        m = _collect_metrics(out_pdf, True, note=\"pytesseract_fallback\")\n",
    "        metrics_json.write_text(json.dumps(m, ensure_ascii=False, indent=2), encoding=\"utf-8\")\n",
    "        return OcrResult(out_pdf, m)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5) make embeddings dimension-safe (tiny edit)\n",
    "\n",
    "In **core/index.py** (where the embedder & Qdrant client are created), ensure:\n",
    "\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import Distance, VectorParams\n",
    "from core.config import CFG\n",
    "\n",
    "def _embed_dim(m):\n",
    "    try:\n",
    "        return m.get_sentence_embedding_dimension()\n",
    "    except Exception:\n",
    "        return len(m.encode(\"probe\", normalize_embeddings=True))\n",
    "\n",
    "embedder = SentenceTransformer(CFG.embed_model, trust_remote_code=True)\n",
    "_raw = _embed_dim(embedder)\n",
    "_DIM = min(CFG.embed_dim or _raw, _raw)\n",
    "\n",
    "client = QdrantClient(url=CFG.qdrant_url)\n",
    "names = {c.name for c in client.get_collections().collections}\n",
    "if CFG.qdrant_collection not in names:\n",
    "    client.create_collection(\n",
    "        collection_name=CFG.qdrant_collection,\n",
    "        vectors_config=VectorParams(size=_DIM, distance=Distance.COSINE),\n",
    "    )\n",
    "\n",
    "def encode_doc(text: str):\n",
    "    v = embedder.encode(CFG.embed_doc_prefix + text, normalize_embeddings=True)\n",
    "    return v[:_DIM]\n",
    "\n",
    "def encode_query(text: str):\n",
    "    v = embedder.encode(CFG.embed_query_prefix + text, normalize_embeddings=True)\n",
    "    return v[:_DIM]\n",
    "```\n",
    "\n",
    "Use `encode_doc/encode_query` wherever you embed.\n",
    "\n",
    "---\n",
    "\n",
    "# 6) call OCR + cleaner in ingest (2 lines)\n",
    "\n",
    "In **scripts/ingest.py** (where you parse PDFs):\n",
    "\n",
    "```python\n",
    "from core.ocr import ocr_pdf_if_needed\n",
    "from core.text_cleaning import clean_text\n",
    "# ...\n",
    "ocr = ocr_pdf_if_needed(pdf_path)                 # <-- NEW\n",
    "raw_text = extract_text_with_pymupdf(ocr.pdf_path)\n",
    "text = clean_text(raw_text)                       # <-- NEW\n",
    "# then your existing chunk -> embed(encode_doc) -> upsert(payload)\n",
    "# include ocr_applied/total_chars in payload if you wish\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7) start qdrant and run the pipeline\n",
    "\n",
    "```powershell\n",
    "docker compose up -d\n",
    "Invoke-RestMethod http://127.0.0.1:6333/healthz\n",
    "\n",
    "# 1) ingest (OCR + parsing + cleaning)\n",
    "python scripts/ingest.py\n",
    "\n",
    "# 2) embed (creates 'tender_docs_jina-v3_d1024_fresh' with correct dim and upserts)\n",
    "python scripts/embed.py\n",
    "```\n",
    "\n",
    "**verify collection populated**\n",
    "\n",
    "```powershell\n",
    "$COLL = \"tender_docs_jina-v3_d1024_fresh\"\n",
    "Invoke-RestMethod -Method Post `\n",
    "  -Uri \"http://127.0.0.1:6333/collections/$COLL/points/count\" `\n",
    "  -ContentType \"application/json\" -Body '{ \"exact\": true }'\n",
    "```\n",
    "\n",
    "**quick retrieval smoke test**\n",
    "\n",
    "```powershell\n",
    "python - << 'PY'\n",
    "from core.qa import retrieve_candidates\n",
    "from core.config import CFG\n",
    "hits = retrieve_candidates(\"Vergabe Frist Anforderungen\", CFG)[:3]\n",
    "for h in hits:\n",
    "    print(round(h.score,4), (h.payload or {}).get(\"source_path\",\"\"), (h.text or \"\")[:100])\n",
    "PY\n",
    "```\n",
    "\n",
    "**launch UI**\n",
    "\n",
    "```powershell\n",
    "streamlit run ui/app_streamlit.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# switches you can try later (no refactor)\n",
    "\n",
    "* **Speed**: set `CFG.embed_dim=512` (Matryoshka crop) and `CFG.ocr_quality=\"speed\"`.\n",
    "* **Precision**: enable reranker (e.g., bge-reranker-v2-m3) in your `core/qa` after dense retrieval.\n",
    "* **GPU OCR**: when you want faster scan OCR, we can add PaddleOCR GPU as a toggle while keeping OCRmyPDF as first choice.\n",
    "\n",
    "that’s it—env **mllocalag** + fresh ingest/parse/embed/retrieve/UI, built for quality first. run the steps and you’ll have a new, clean collection with better text and robust imports.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db64017",
   "metadata": {},
   "source": [
    "# update requirement file,\n",
    "\n",
    "\n",
    "# ---- Vector DB ----\n",
    "qdrant-client>=1.9,<2.0\n",
    "\n",
    "# ---- Embeddings / HF stack (needed for Jina v3 remote code) ----\n",
    "sentence-transformers>=3.0.1\n",
    "transformers>=4.44,<5\n",
    "huggingface_hub>=0.24\n",
    "accelerate>=0.33\n",
    "safetensors>=0.4.3\n",
    "FlagEmbedding>=1.2.10     # keep if you'll use a reranker later (ok to leave)\n",
    "\n",
    "# ---- Data / PDF / utils ----\n",
    "numpy>=1.24,<3\n",
    "pandas>=2.2\n",
    "openpyxl>=3.1\n",
    "pymupdf>=1.24\n",
    "pdfplumber>=0.11\n",
    "tqdm>=4.66\n",
    "langdetect>=1.0.9\n",
    "python-magic-bin==0.4.14 ; sys_platform == \"win32\"\n",
    "\n",
    "# ---- OCR (quality-first path) ----\n",
    "ocrmypdf>=16.0\n",
    "pytesseract>=0.3.10\n",
    "Pillow>=10.0\n",
    "\n",
    "# ---- LLM / UI ----\n",
    "ollama>=0.3.0\n",
    "streamlit>=1.36.0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc908c1",
   "metadata": {},
   "source": [
    "# update ingest and io\n",
    "\n",
    "Why these versions\n",
    "\n",
    "Works with your Pydantic config (all paths are Path).\n",
    "\n",
    "PDFLoader uses OCR fallback (pytesseract, 300 DPI) and cleans text for better recall.\n",
    "\n",
    "Ingest handles ZIPs + loose files, logs to data/logs/ingest_YYYY-MM.csv.\n",
    "\n",
    "Excel metadata is optional and cleaned recursively from data/raw/.\n",
    "\n",
    "You’re good to run:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57d2e32",
   "metadata": {},
   "source": [
    "# delete old collection # %%\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def delete_qdrant_collection():\n",
    "    \"\"\"Delete existing Qdrant collection\"\"\"\n",
    "    \n",
    "    try:\n",
    "        from qdrant_client import QdrantClient\n",
    "        from core.config import CFG\n",
    "        \n",
    "        print(\"\\n🗑️ Connecting to Qdrant...\")\n",
    "        client = QdrantClient(url=CFG.qdrant_url)\n",
    "        \n",
    "        collections = client.get_collections().collections\n",
    "        collection_names = [c.name for c in collections]\n",
    "        \n",
    "        if CFG.qdrant_collection in collection_names:\n",
    "            print(f\"🗑️ Deleting collection: {CFG.qdrant_collection}\")\n",
    "            client.delete_collection(CFG.qdrant_collection)\n",
    "            print(\"✅ Qdrant collection deleted successfully\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"ℹ️ No collection named '{CFG.qdrant_collection}' found\")\n",
    "            return False\n",
    "            \n",
    "    except ImportError:\n",
    "        print(\"⚠️ qdrant_client not available - install with: pip install qdrant-client\")\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Could not delete Qdrant collection: {e}\")\n",
    "        return False\n",
    "    \n",
    "def main():\n",
    "    print(\"🚀 Starting Nuclear Cleanup\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    \n",
    "    # Step 2: Delete Qdrant collection\n",
    "    qdrant_deleted = delete_qdrant_collection()\n",
    "    \n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 50)\n",
    "    print(\"🎉 NUCLEAR CLEANUP COMPLETE!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"✅ Unwanted script/UI files removed\")\n",
    "    print(\"✅ Data directories cleaned\")\n",
    "    print(f\"{'✅' if qdrant_deleted else '⚠️'} Qdrant collection {'deleted' if qdrant_deleted else 'not found/accessible'}\")\n",
    "    print(\"\\n🚀 Ready for unified document processing!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1163ed18",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
